{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio==3.50.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Whisper model\n",
    "whisper_model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer for retrieval\n",
    "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy RAG document store\n",
    "documents = {\n",
    "    \"doc1\": \"This is a document about artificial intelligence and machine learning.\",\n",
    "    \"doc2\": \"This document describes the basics of deep learning and neural networks.\",\n",
    "    \"doc3\": \"Here we discuss the impact of AI on different industries like healthcare, finance, and more.\",\n",
    "    \"doc4\": \"The future of technology includes advancements in AI, quantum computing, and other fields.\",\n",
    "    \"doc5\": [\n",
    "        \"The future of AI is incredibly promising, marked by rapid advancements and transformative potential across various sectors.\",\n",
    "        \"AI is expected to revolutionize industries such as healthcare, finance, transportation, and education by enabling more efficient and accurate decision-making processes.\",\n",
    "        \"In healthcare, AI can aid in early diagnosis, personalized treatment plans, and even robotic surgeries.\",\n",
    "        \"In finance, AI-driven algorithms can detect fraudulent activities and optimize trading strategies.\",\n",
    "        \"The transportation sector is likely to see the widespread adoption of autonomous vehicles, improving safety and reducing congestion.\",\n",
    "        \"Education will benefit from personalized learning experiences tailored to individual student needs.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten the document content for encoding\n",
    "all_document_texts = []\n",
    "for key, value in documents.items():\n",
    "    if isinstance(value, list):\n",
    "        all_document_texts.extend(value)\n",
    "    else:\n",
    "        all_document_texts.append(value)\n",
    "\n",
    "# Encode the documents using the retriever model\n",
    "document_embeddings = retriever_model.encode(all_document_texts, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to transcribe speech using Whisper\n",
    "def transcribe_speech(file_path):\n",
    "    result = whisper_model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(audio_path):\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "    _, probs = whisper_model.detect_language(mel)\n",
    "    detected_language_code = max(probs, key=probs.get)\n",
    "\n",
    "    # Map detected language codes to readable names\n",
    "    language_mapping = {\n",
    "        'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',\n",
    "        'hi': 'Hindi', 'ja': 'Japanese', 'ru': 'Russian', 'ar': 'Arabic',\n",
    "        'te': 'Telugu', 'zh': 'Chinese', 'pt': 'Portuguese'\n",
    "    }\n",
    "\n",
    "    return language_mapping.get(detected_language_code, detected_language_code).capitalize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load translation model and tokenizer\n",
    "def load_translation_model(language):\n",
    "    model_name = {\n",
    "        \"Hindi\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
    "        \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "        \"Japanese\": \"Helsinki-NLP/opus-mt-en-jap\",\n",
    "        \"German\": \"Helsinki-NLP/opus-mt-en-de\",\n",
    "        \"Russian\": \"Helsinki-NLP/opus-mt-en-ru\",\n",
    "        \"Arabic\": \"Helsinki-NLP/opus-mt-en-ar\",\n",
    "        \"Telugu\": \"Helsinki-NLP/opus-mt-en-te\",\n",
    "        \"French\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "        \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "        \"English\": \"Helsinki-NLP/opus-mt-xx-en\"  # Used for translating any language to English\n",
    "    }\n",
    "\n",
    "    if language not in model_name:\n",
    "        raise ValueError(f\"Translation model for {language} not available.\")\n",
    "\n",
    "    translation_model = MarianMTModel.from_pretrained(model_name[language])\n",
    "    translation_tokenizer = MarianTokenizer.from_pretrained(model_name[language])\n",
    "    return translation_model, translation_tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate text\n",
    "def translate_text(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = model.generate(**inputs)\n",
    "    translation = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to retrieve a document based on the query\n",
    "def retrieve_document(query):\n",
    "    query_embedding = retriever_model.encode(query, convert_to_tensor=True)\n",
    "    scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]\n",
    "    top_score_idx = scores.argmax().item()\n",
    "    return list(documents.values())[top_score_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the audio file and return transcriptions and translations\n",
    "def process_audio(audio, target_language):\n",
    "    # Transcribe the audio\n",
    "    transcription = transcribe_speech(audio)\n",
    "\n",
    "    # Detect the language spoken in the audio\n",
    "    detected_language = detect_language(audio)\n",
    "\n",
    "    # Load the appropriate translation model\n",
    "    translation_model, translation_tokenizer = load_translation_model(target_language)\n",
    "\n",
    "    # Translate the transcribed text\n",
    "    translated_text = translate_text(transcription, translation_model, translation_tokenizer)\n",
    "\n",
    "    # Retrieve document based on the transcribed text\n",
    "    retrieved_document = retrieve_document(transcription)\n",
    "\n",
    "    return transcription, detected_language, translated_text, retrieved_document\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"upload\", type=\"filepath\"),\n",
    "        gr.Dropdown([\"Hindi\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Arabic\", \"French\", \"Italian\", \"English\"], label=\"Target Language\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Transcription\"),\n",
    "        gr.Textbox(label=\"Detected Language\"),\n",
    "        gr.Textbox(label=\"Translation\"),\n",
    "        gr.Textbox(label=\"Retrieved Document\")\n",
    "    ],\n",
    "    title=\"Multilingual Speech Recognition, Translation, and Document Retrieval\",\n",
    "    description=\"Upload an audio file in any language, select a target language to get the transcription, translation, and retrieve a document based on the transcription.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
